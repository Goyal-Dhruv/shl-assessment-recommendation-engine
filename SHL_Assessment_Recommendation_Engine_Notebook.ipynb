SHL – Assessment Recommendation Engine
# If running locally, install deps once:
# !pip install pandas numpy scikit-learn requests beautifulsoup4 lxml

import re
import json
import pandas as pd
import numpy as np

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import os

# Path where you'll keep catalog.csv
CATALOG_PATH = "data/catalog.csv"

os.makedirs("data", exist_ok=True)

# If you built 'df' from scraping, save it as starter CSV
if 'df' in globals() and isinstance(df, pd.DataFrame) and len(df)>0:
    df.to_csv(CATALOG_PATH, index=False)
    print("Saved starter catalog to", CATALOG_PATH)

# Load catalog
catalog = pd.read_csv(CATALOG_PATH)
catalog.head()

Build the recommender (TF‑IDF + cosine similarity + small rule boosts)
def normalize_text(s: str) -> str:
    s = s or ""
    s = re.sub(r"\s+", " ", s.strip())
    return s

# Combine fields into one searchable document
catalog = catalog.fillna("")
catalog["doc"] = (
    catalog["name"].astype(str) + " \n" +
    catalog["description"].astype(str) + " \n" +
    catalog["skills"].astype(str) + " \n" +
    catalog["category"].astype(str) + " \n" +
    catalog["job_levels"].astype(str)
).map(normalize_text)

vectorizer = TfidfVectorizer(
    stop_words="english",
    ngram_range=(1,2),
    min_df=1,
    max_features=40000
)

X = vectorizer.fit_transform(catalog["doc"])  # matrix of assessments

def rule_boost(query: str, row: pd.Series) -> float:
    """Small additive boost based on simple heuristics."""
    q = (query or "").lower()
    cat = (row.get("category", "") or "").lower()
    skills = (row.get("skills", "") or "").lower()
    lvl = (row.get("job_levels", "") or "").lower()
    
    boost = 0.0
    
    # Leadership / seniority hints
    if any(k in q for k in ["manager", "lead", "leadership", "stakeholder", "strategy"]):
        if any(k in cat for k in ["behavior", "sjt", "personality"]):
            boost += 0.03
    
    # Data / analytics hints
    if any(k in q for k in ["data", "analytics", "sql", "python", "statistics", "ml", "machine learning"]):
        if any(k in skills for k in ["problem", "reasoning", "analytical", "numerical", "data"]):
            boost += 0.03
        if "cognitive" in cat:
            boost += 0.02
    
    # Communication / language hints
    if any(k in q for k in ["english", "communication", "writing", "grammar"]):
        if "language" in cat or "english" in skills:
            boost += 0.05
    
    # Entry level hints
    if any(k in q for k in ["fresher", "entry", "junior", "graduate"]):
        if "entry" in lvl.lower():
            boost += 0.03
    
    return boost

def top_contributing_terms(query: str, row_doc: str, top_n: int = 6):
    """Explain recommendation via overlapping TF-IDF terms (simple, fast)."""
    q_vec = vectorizer.transform([query])
    r_vec = vectorizer.transform([row_doc])
    # elementwise product -> contributions
    contrib = q_vec.multiply(r_vec)
    if contrib.nnz == 0:
        return []
    # get top indices by weight
    coo = contrib.tocoo()
    pairs = sorted(zip(coo.col, coo.data), key=lambda x: x[1], reverse=True)[:top_n]
    terms = [vectorizer.get_feature_names_out()[i] for i,_ in pairs]
    return terms

def recommend(job_title: str = "", skills=None, job_description: str = "", top_k: int = 10):
    skills = skills or []
    query = " ".join([job_title, job_description, " ".join(skills)]).strip()
    if not query:
        raise ValueError("Provide at least job_title or skills or job_description.")
    
    q_vec = vectorizer.transform([query])
    sims = cosine_similarity(q_vec, X).ravel()
    
    # Apply rule boosts
    boosts = np.array([rule_boost(query, catalog.iloc[i]) for i in range(len(catalog))])
    final = sims + boosts
    
    idx = np.argsort(-final)[:top_k]
    out = catalog.iloc[idx][["assessment_id","name","url","category","job_levels","skills","description"]].copy()
    out["score"] = final[idx]
    out["why"] = [", ".join(top_contributing_terms(query, catalog.iloc[i]["doc"])) for i in idx]
    return out.reset_index(drop=True)

# Quick sanity check
recommend(job_title="Data Analyst Intern", skills=["SQL","Excel","Python","statistics"], job_description="" , top_k=5)

queries = [
    dict(job_title="Research AI Intern", skills=["NLP","machine learning","python","experimentation"],
         job_description="Build models, evaluate trade-offs, explain results", top_k=10),
    dict(job_title="Customer Support Associate", skills=["English communication","typing","attention to detail"],
         job_description="Handle chats and emails, quality writing", top_k=10),
    dict(job_title="Sales Executive", skills=["communication","negotiation","customer handling"],
         job_description="Meet targets, handle objections", top_k=10)
]

for q in queries:
    print("\n===", q['job_title'], "===")
    display(recommend(**q).head(5))

out = recommend(job_title="Data Analyst Intern", skills=["SQL","Excel","Python","statistics"], top_k=10)
out.to_csv("sample_recommendations.csv", index=False)
print("Saved: sample_recommendations.csv")
out

# This cell writes a minimal FastAPI app to src/api.py
# Run locally:
#   uvicorn src.api:app --reload --port 8000

api_code = r'''
from fastapi import FastAPI
from pydantic import BaseModel
import pandas as pd
import numpy as np
import re

import faiss
from sentence_transformers import SentenceTransformer
from pathlib import Path

# ---------- Paths ----------
ROOT = Path(__file__).resolve().parents[1]
INDEX_PATH = ROOT / "data" / "catalog.index"
META_PATH  = ROOT / "data" / "catalog.pkl"

# ---------- App ----------
app = FastAPI(title="SHL Assessment Recommendation Engine (RAG)")

class RecommendRequest(BaseModel):
    job_title: str = ""
    skills: list[str] = []
    job_description: str = ""
    top_k: int = 10

def normalize_text(s: str) -> str:
    s = s or ""
    return re.sub(r"\s+", " ", s.strip())

# ---------- Load catalog + embedder + FAISS ----------
catalog = pd.read_pickle(META_PATH).fillna("")
embedder = SentenceTransformer("all-MiniLM-L6-v2")
index = faiss.read_index(str(INDEX_PATH))

def rule_boost(query: str, row: pd.Series) -> float:
    q = (query or "").lower()
    cat = (row.get("category", "") or "").lower()
    skills = (row.get("skills", "") or "").lower()
    lvl = (row.get("job_levels", "") or "").lower()

    boost = 0.0

    # ---------- Leadership / manager intent ----------
    if any(k in q for k in ["manager", "lead", "leadership", "stakeholder", "strategy"]):
        if any(k in cat for k in ["behavior", "sjt", "personality", "job focused"]):
            boost += 0.08
        if any(k in lvl for k in ["senior", "manager"]):
            boost += 0.06

    # ---------- AI / Tech intent ----------
    ai_intent = any(k in q for k in ["ai", "ml", "machine learning", "nlp", "deep learning", "data scientist", "research"])
    tech_intent = any(k in q for k in ["python", "sql", "coding", "programming", "developer", "engineer", "data", "analytics", "statistics"])

    if ai_intent or tech_intent:
        # Strong boosts: what we WANT on top for AI intern roles
        if any(k in skills for k in ["coding", "python", "algorithms", "machine learning", "data science", "data engineering", "problem solving"]):
            boost += 0.35

        if "cognitive" in cat:
            boost += 0.25

        if ("skills" in cat) or ("simulation" in cat):
            boost += 0.15

        if any(k in lvl for k in ["entry", "graduate", "intern", "junior"]):
            boost += 0.12

        # Strong penalties: what we DON'T want on top for AI intern roles
        if any(k in skills for k in ["business skills", "computer literacy", "workplace productivity"]):
            boost -= 0.45

        if any(k in cat for k in ["behavioral", "personality", "virtual assessment center"]):
            boost -= 0.30

        if any(k in lvl for k in ["senior", "manager"]):
            boost -= 0.60

        # Language tests: only if explicitly asked
        if ("language" in cat) or ("english" in skills):
            if not any(k in q for k in ["english", "communication", "writing", "grammar", "spoken"]):
                boost -= 0.35

    # ---------- Explicit language intent ----------
    if any(k in q for k in ["english", "communication", "writing", "grammar", "spoken"]):
        if ("language" in cat) or ("english" in skills):
            boost += 0.20

    # ---------- Entry-level intent ----------
    if any(k in q for k in ["fresher", "entry", "junior", "graduate", "intern"]):
        if "entry" in lvl:
            boost += 0.06
        if "graduate" in lvl:
            boost += 0.05

    return boost


@app.get("/health")
def health():
    return {"status": "ok"}

@app.post("/recommend")
def recommend(req: RecommendRequest):
    query = " ".join([req.job_title, req.job_description, " ".join(req.skills)]).strip()
    if not query:
        return {"error": "Provide job_title or job_description or skills"}

    # Embed query (cosine similarity via normalized embeddings + inner product index)
    q_emb = embedder.encode([query], normalize_embeddings=True)
    q_emb = np.asarray(q_emb, dtype="float32")

    k = max(1, int(req.top_k))
    scores, idxs = index.search(q_emb, k)  # scores: (1,k), idxs: (1,k)

    results = []
    for rank, i in enumerate(idxs[0], 1):
        row = catalog.iloc[int(i)].to_dict()

        boost = rule_boost(query, pd.Series(row))
        final_score = float(scores[0][rank - 1] + boost)

        desc = row.get("description", "") or ""
        evidence = desc[:220] + ("..." if len(desc) > 220 else "")

        results.append({
            "rank": rank,
            "assessment_id": row.get("assessment_id",""),
            "name": row.get("name",""),
            "url": row.get("url",""),
            "category": row.get("category",""),
            "job_levels": row.get("job_levels",""),
            "skills": row.get("skills",""),
            "score": round(final_score, 4),
            "evidence": evidence
        })
        results = sorted(results, key=lambda x: x["score"], reverse=True)
        for j, r in enumerate(results, 1):
            r["rank"] = j

    return {"query": query, "results": results}

@app.post("/recommend/pretty")
def recommend_pretty(req: RecommendRequest):
    out = recommend(req)
    if "results" not in out:
        return out

    lines = []
    for r in out["results"]:
        lines.append(
            f"{r['rank']}. {r['name']} ({r['category']} | {r['job_levels']})\n"
            f"   Evidence: {r.get('evidence','')}\n"
            f"   Link: {r['url']}\n"
        )

    return {
        "query": out["query"],
        "summary": "\n".join(lines),
        "results": out["results"]
    }

    
'''
import os
os.makedirs("src", exist_ok=True)
with open("src/api.py", "w", encoding="utf-8") as f:
    f.write(api_code)

print("Wrote src/api.py")

